{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験3-2 次元圧縮\n",
    "\n",
    "一般に高次元のデータは，可視化などが困難であるため，直感的にデータを理解することが難しいです．演習１で低次元（２次元）のデータを取り扱ったのも，直感的な理解を深めるためです．\n",
    "その一方で，一般的なデータは高次元の場合がほとんどです．\n",
    "そこで，高次元のデータを可視化したり，低次元に射影して使われる教師なし学習方法が次元圧縮と言われているものになります．\n",
    "データを（２次元空間などの）低次元へ射影することで，可視化したり，データが表現される有効な空間のなかでの取り扱いをすることで，機械学習のアルゴリズムの効率を上げたりすることが次元圧縮の趣旨になります．\n",
    "\n",
    "次元圧縮の方法としては，いくつかのやり方があります\n",
    "\n",
    "- 主成分分析 （Principal Component Analysis: PCA）\n",
    "- t-SNE, UMAP 局所構造を保ちながら次元を圧縮\n",
    "\n",
    "などが比較的有名な方法です．ここでは簡単な主成分分析を取り上げます．\n",
    "主成分分析は，データが最も拡がっている線形部分空間を取り出して，そこにデータ点を射影します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ３次元に埋め込まれたデータの主成分分析\n",
    "\n",
    "まずは手始めに，視覚的な理解を目的として３次元空間に埋め込まれたデータを主成分分析を用いて２次元に射影して，次元圧縮を考えます．\n",
    "\n",
    "下記のデータでは，３次現状の渦巻き構造を考えます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データ生成 (3次元空間に埋め込まれたスパイラル)\n",
    "def generate_3d_spiral_data(n_samples=100, magnitude=1, noise=0.05):\n",
    "    \"\"\"\n",
    "    3次元空間に埋め込まれたスパイラルデータを生成する\n",
    "    n_samples : int\n",
    "        データ数\n",
    "    magnitude : float\n",
    "        スパイラルの開き具合\n",
    "    noise : float\n",
    "        gaussian noiseの標準偏差\n",
    "    生成データ，媒介変数θ，ノイズなし生成データを返す．\n",
    "    なお各データは(n_samples, 3)の形状を持つ\n",
    "    \"\"\"\n",
    "    theta = np.linspace(0, 4 * np.pi, n_samples)  # 角度\n",
    "    z = np.linspace(0, 1, n_samples)  # 高さ\n",
    "    r = magnitude * z  # 半径\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    true_data_3d = np.vstack((x, y, z)).T\n",
    "\n",
    "    # データにノイズを載せる\n",
    "    data_3d = true_data_3d + np.random.randn(n_samples*3).reshape(n_samples, 3) * noise\n",
    "    return theta, true_data_3d, data_3d\n",
    "\n",
    "\n",
    "N = 500 # データ数\n",
    "a = 0.9 # 螺旋の開き具合\n",
    "sd = 0.02 # データにノイズを載せる(0.05 くらいにすると面白い)\n",
    "\n",
    "theta, true_data_3d, data_3d = generate_3d_spiral_data(n_samples=N, magnitude=a, noise=sd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで `data_3d` に，３次元で５００個のサンプルデータが生成されます．\n",
    "データは $R^3 \\in \\{(x, y, z)\\mid [-1, 1] \\times [-1, 1] \\times [0, 1]\\}$ な空間に埋め込まれているはずなので\n",
    "可視化してみます．\n",
    "パラメータ `a` をコントロールすることで，渦巻きの $x-y$ 方向のサイズを変えることができることを確認してみてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# 3D plot\n",
    "ax0 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "ax0.scatter(data_3d[:, 0], data_3d[:, 1], data_3d[:, 2], c=theta, s=10, cmap='viridis')\n",
    "ax0.set_title(\"Original 3D Data\")\n",
    "\n",
    "# X-Y plot\n",
    "ax1 = fig.add_subplot(2, 2, 2)\n",
    "ax1.scatter(data_3d[:, 0], data_3d[:, 1], c=theta, cmap='viridis', s=10)\n",
    "ax1.set_title(\"X-Y plot\")\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_xlim(-1, 1)\n",
    "ax1.set_ylim(-1, 1)\n",
    "\n",
    "# X-Z plot\n",
    "ax2 = fig.add_subplot(2, 2, 3)\n",
    "ax2.scatter(data_3d[:, 0], data_3d[:, 2], c=theta, cmap='viridis', s=10)\n",
    "ax2.set_title(\"X-Z plot\")\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_xlim(-1, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Y-Z plot\n",
    "ax3 = fig.add_subplot(2, 2, 4)\n",
    "ax3.scatter(data_3d[:, 1], data_3d[:, 2], c=theta, cmap='viridis', s=10)\n",
    "ax3.set_title(\"Y-Z plot\")\n",
    "ax3.set_aspect('equal')\n",
    "ax3.set_xlim(-1, 1)\n",
    "ax3.set_ylim(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA による主成分分析\n",
    "\n",
    "主成分分析は，データ全体から，その拡がった方向を抽出します．\n",
    "データ自体から広がり方という特性を取り出すことから，教師なし学習と捉えることができます．\n",
    "実装自体は，さまざまなプラットフォームで実装されていますが，ここでは `scikit-learn` に実装されたもの(`sklearn.decomposition.PCA`: 以下 `PCA` クラスと呼びます)を使っていきます．\n",
    "\n",
    "`PCA` クラスではインスタンスを作成する場合，どのくらいに次元を落とすかを指定します．\n",
    "ここでは，可視化するために２次元を指定します．\n",
    "\n",
    "`scikit-learn` を用いる場合，通常 `fit()` 関数を使うことで学習が行われます．\n",
    "さらにデータを与えて，特性に合わせた変換を行うには， `fit_transform()` を用います．\n",
    "主成分分析を用いた場合は，拡がった次元方向を特定し，データをその次元へ射影します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCAで次元圧縮 (2次元)\n",
    "pca = PCA(n_components=2)\n",
    "data_2d = pca.fit_transform(data_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(data_2d[:, 0], data_2d[:, 1], s=10)\n",
    "ax.set_title(\"PCA\")\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元を大きくして一般化してみる\n",
    "\n",
    "先ほどは３次元の可視化できるデータをランダムなデータを策定して，上述の渦巻き構造を 100 次元に埋め込まれた３次元の話として考えてみます．\n",
    "データの生成としては，先ほどの螺旋状の３次元データを作成し，これを１０次元空間に埋め込んで，あとは適当な直交座標変換することで，人工データを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def embed_high_dimension(embed_data, dim=100, noise=0.05):\n",
    "    \"\"\"\n",
    "    dim 次元空間に埋め込む\n",
    "    \"\"\"\n",
    "    N = embed_data.shape[0] # データ数\n",
    "    true_M = embed_data.shape[1] # 意味のある次元数\n",
    "    M_rest = dim - true_M # 意味のない次元数\n",
    "    \n",
    "    # 高次元のデータを生成\n",
    "    data_high_dim = np.zeros((N, dim))\n",
    "    random_data = np.random.randn(N*M_rest).reshape(N, M_rest) * noise\n",
    "    data_high_dim[:, :3] = embed_data\n",
    "    data_high_dim[:, 3:] = random_data\n",
    "    return data_high_dim\n",
    "\n",
    "def apply_random_rotation(data, noise=0.05):\n",
    "    \"\"\"\n",
    "    データにランダムな回転を適用する\n",
    "    \"\"\"\n",
    "    N, M = data.shape\n",
    "    random_matrix = np.random.randn(M*M).reshape(M, M) * noise\n",
    "    q, _ = np.linalg.qr(random_matrix) # QR 分解で直交行列に変換\n",
    "    permuted_data = data @ q\n",
    "\n",
    "    return permuted_data, q\n",
    "\n",
    "# データ生成 (100次元空間に埋め込まれたスパイラル)\n",
    "M = 100 # 次元数\n",
    "N = 500 # データ数\n",
    "a = 0.9 # 螺旋の開き具合\n",
    "\n",
    "# 螺旋の方程式\n",
    "theta, true_data, data_3d = generate_3d_spiral_data(n_samples=N, magnitude=a, noise=sd)\n",
    "# とりあえず１００次元空間の最初の列に埋め込む\n",
    "data_highdim = embed_high_dimension(data_3d, dim=M, noise=sd)\n",
    "# データをランダム直交で変換して混ぜる\n",
    "synth_data, q = apply_random_rotation(data_highdim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでもう，データがどう埋め込まれているか，わからなくなりました．\n",
    "100次元全ては，次元が高すぎてすべて見れないですが，最初の10次元の特徴における散布図ぐらいまでは，ペアプロットで眺めることができます．\n",
    "ペアプロットは `matplotlib` でつくることもできるのですが，面倒なので `seaborn` モジュールの機能を使います．このためにデータを一度 `pandas` のデータフレームに変換し，\n",
    "それを `seaborn.pairplot` に突っ込みます．\n",
    "\n",
    "\n",
    "なお，通常のデータサイエンス研究や機械学習研究では，このような構造が見えないような（隠されている）ものを与えられるのが一般的です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix # pandas の pairplot を使う\n",
    "\n",
    "# 10次元分のデータフレームつくってペアプロットしてみる\n",
    "df = pd.DataFrame(synth_data[:, :10])\n",
    "#sns.pairplot(df)\n",
    "_ = scatter_matrix(df, alpha=0.8, figsize=(10, 10), diagonal='hist')\n",
    "\n",
    "# たぶん20次元くらいまでは pairplot で概観できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なにか構造ぽいものが，含まれているのがわかるかと思いますが，あまり明確ではないかと思います．\n",
    "（sd の値を 0.05 程度にするとどの散布図も，みんな同じような正規分布に見えるようになってきます）\n",
    "\n",
    "ただし，この場合，混ぜた直交行列 `q` がわかっているので，転置行列（逆行列でもある） `q.T` をかければ，もとの埋め込んだデータを復号できます．\n",
    "下記のコードでみてみると，ちゃんと最初の3次元分にらせん構造が埋め込まれているのが下記で確認できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もとのデータは q の逆行列（この場合は q.T）をかける\n",
    "reconstruct_data = synth_data @ q.T\n",
    "\n",
    "df2 = pd.DataFrame(reconstruct_data[:, :10])\n",
    "# sns.pairplot(df2)\n",
    "_ = scatter_matrix(df2, alpha=0.8, figsize=(10, 10), diagonal='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて，それでは，この `synthesized_data` から，PCA を用いてデータの構造を取り出すことができるかを見ていきます．\n",
    "とりあえず次元数 (この場合 100) 次元で PCA をかけて，上位2つの軸を用いて散布図をとってみます．\n",
    "これらの軸は *主成分* の軸と呼ばれ，データが定義されている空間で，拡がっている方向順に，第1主成分(The 1st Principal Component: PC1)，第2主成分(The 2nd Princial Compoent: PC2) と呼ばれます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca_decomposed = pca.fit_transform(synth_data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(pca_decomposed[:, 0], pca_decomposed[:, 1], s=10)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.grid()\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしくは，先ほどと同じ様に，PCAの各次元のペアプロットで概観をみることができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もしくは pairplot で概観する\n",
    "df3 = pd.DataFrame(pca_decomposed[:, :10])\n",
    "# sns.pairplot(df3)\n",
    "_ = scatter_matrix(df3, alpha=0.8, figsize=(10, 10), diagonal='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの中で，渦の構造が一番幅広く変化している場合は，うまく取り出せます．\n",
    "\n",
    "それではデータが有効な次元はどのくらいかを見てみます．これは，データ全体のなかで，PCAの各次元がどの程度拡がっているかの割合をみることで指標を得ることができます．\n",
    "これは *累積寄与率* と呼ばれる量になります．累積寄与率の計算は PCA の計算で出てくる固有値の割合を調べればよいのですが，ここらへんは多変量解析などの教科書に任せます．\n",
    "`sklearn.decomposition` 内の `PCA` クラスの中では `explained_variance_ratio_` と呼ばれる値に，pca を行った各次元の寄与（固有値）が計算されていますので，\n",
    "これの累積割合を出してあげます．\n",
    "各次元の寄与を $v_m$ ($m = 1 \\cdots M$) とすると $c_m = \\frac{\\sum_{j=1}^{m} v_j}{\\sum_{k=1}^{M} v_k}$ という，$m$ 番目までの累積寄与は `numpy` の `cumsum` で計算できるのでやってみます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(cumulative_explained_variance_ratio)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Number of components\")\n",
    "ax.set_ylabel(\"Cumulative explained variance ratio\")\n",
    "ax.set_title(\"PCA explained variance ratio\")\n",
    "\n",
    "# ax.set_xlim(0, 10) # 10次元までの累積寄与率を見る\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをみると，最初の3つの要素だけで全体の9割を占めており，あとは徐々に上がっていくのが見えますので，最初の3つが重要そうというのが感覚的にわかります．\n",
    "データへのノイズをあげると，この部分が上がったり下がったりします（sd = 0.05 にすると６割り程度）\n",
    "累積寄与率が9割（9割のデータの広がり方を説明できる）までの次元をとるといった方法などは，割と標準的に次元を削減するのに使われます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE による次元圧縮\n",
    "\n",
    "*t-SNE* や *UMAP* と呼ばれる手法は，主にデータを可視化するための次元圧縮方法です．\n",
    "先ほどの主成分分析とは異なり，局所的なデータの近さの分布に基づいてマッピングするため，全体の構造を眺めることはできませんが，局所的なデータの構造を2次元に埋め込んで概観することができます．\n",
    "先ほどの合成データを眺めてみます．\n",
    "\n",
    "なお，t-SNE の場合，可視化が目的なので，圧縮する次元 `n_components` の値を4以上にすると警告が出てきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100次元のデータを t-SNE で2次元に圧縮\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne_decomposed = tsne.fit_transform(synth_data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(tsne_decomposed[:, 0], tsne_decomposed[:, 1], s=10)\n",
    "ax.set_title(\"t-SNE analysis\")\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうやら，なにか構造があって，1次元ぽくも見えます．\n",
    "上からデータの出現順（`theta` に相当）してみるともっとはっきりと構造が見えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.linspace(0, 1, N) # データの出現順に色をつける（theta に相当するが，データからとりだす）\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(tsne_decomposed[:, 0], tsne_decomposed[:, 1], c=index, cmap='viridis', s=10)\n",
    "ax.set_title(\"t-SNE indexed by data order\")\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "色の変化度合いからすると，構造的には1次元のものが曲がって100次元データの中にうめ込まれていて，\n",
    "それを t-SNE で2次元にマッピングすると，その構造がほどかれて，可視化されていそうというのが見て取れます．さらに黄色のほうがデータの散らばりが少なく，濃い青のほうが散らばりが多いことからデータの最初と終わりとではデータの散らばり方が違うことも見えてきます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて，ここまでは，標準の t-SNE の使い方を見てきたわけですが，t-SNE 自体は *教師なし学習* の一種で，その振る舞いを変える（ハイパー）パラメータが存在します．\n",
    "結果を大きく左右するハイパーパラメータは perplexity と呼ばれる値で，大雑把に言えば k-means 法のクラスタ数に該当するパラメータになります．\n",
    "このアルゴリズムを提唱した Maaten と Hinton は，5~50 程度にしなさいと言っています．\n",
    "\n",
    "以下に perplexity を 1~100 に変化させたものを図示します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100次元のデータを t-SNE で2次元に圧縮\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "perplexities = [1, 5, 10, 50, 99]\n",
    "index = np.linspace(0, 1, synth_data.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, len(perplexities), figsize=(15, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, p in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, perplexity=p)\n",
    "    tsne_decomposed = tsne.fit_transform(synth_data)\n",
    "\n",
    "    ax[i].scatter(tsne_decomposed[:, 0], tsne_decomposed[:, 1], c=index, cmap='viridis', s=10)\n",
    "    ax[i].set_title(f\"t-SNE (perplexity={p})\")\n",
    "    ax[i].set_aspect('equal')\n",
    "    ax[i].grid()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity の違いによって，同じデータを用いてもぜんぜん違う可視化結果が得られるのがわかると思います．\n",
    "この値が小さいとよくわからない配置になります．なので適切に設定する必要があります．（`sklearn` のデフォルトでは 30 に設定されている）\n",
    "また，純粋なノイズを与えた場合にも，この値の設定次第で構造ぽくみえるようになるケースもあります．\n",
    "PCAの場合は，データを見る角度を変えているだけなのですが，この場合は空間の伸縮させてもいるので，点の近さどうしはデータ点同士の密度の意味でしか当てになりません．\n",
    "\n",
    "データを間引いた場合でも，また構造は変わってきます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100次元のデータを t-SNE で2次元に圧縮\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced_data = synth_data[::10] # 1/10 だけ使う\n",
    "\n",
    "perplexities = [1, 5, 10, 49]\n",
    "index = np.linspace(0, 1, reduced_data.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, len(perplexities), figsize=(15, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, p in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, perplexity=p)\n",
    "    tsne_decomposed = tsne.fit_transform(reduced_data)\n",
    "\n",
    "    ax[i].scatter(tsne_decomposed[:, 0], tsne_decomposed[:, 1], c=index, cmap='viridis', s=10)\n",
    "    ax[i].set_title(f\"t-SNE (perplexity={p})\")\n",
    "    ax[i].set_aspect('equal')\n",
    "    ax[i].grid()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は perpexity = 1 に謎のクラスタぽい集団が現れているのがわかります．（実際にはそんなものはない）\n",
    "\n",
    "t-SNE や UMAP の場合は，このように見てくれを変えられる柔軟な（ミスリーディングさせることが可能な）手法であることを考慮したうえで使うことを心がけてください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験3-2\n",
    "\n",
    "1. 上記らせん構造データの広がりパラメータ `a` の大きさを $0 \\sim 1$ の間で制御することによって，どの程度の広がり方 ($a$) があれば，PCA で，らせん構造が取り出せそうかを考察しなさい．\n",
    "2. MNIST (データ次元784) のデータ のうち，1000 個のデータを取り出した上で，PCA で次元圧縮をおこない，２次元で図示しなさい．\n",
    "3. 上述のMNISTデータを t-SNE を用いて可視化し，PCA との違いを考察しなさい．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
