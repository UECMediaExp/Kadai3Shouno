{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験3-1 クラスタリング\n",
    "\n",
    "ここでは，教師あり学習以外の教師なし学習全般を考えていきます．\n",
    "最初は，教師なし学習の応用として， `クラスタリング` を考えてみます．\n",
    "クラスタリングは，与えられたデータに対して，データの集まり度合い（クラスタと呼びます）に応じてグループ化する技術です．\n",
    "\n",
    "まず最初に例題として２次元のデータをクラスタリングすることを考えてみます．\n",
    "\n",
    "クラスタリングには様々な流儀がありますが，ここでは K-平均法 (K-means) を取り扱います．\n",
    "K-平均法の基本アルゴリズムは，\n",
    "\n",
    "1. クラスタ個数 K 個を設定する\n",
    "2. 入力の空間に K 個のクラスタ中心を適当にばらまく\n",
    "3. 各データを最も近いクラスタに中心に属させる\n",
    "4. クラスタの中心を所属しているデータの平均へ移動させる．\n",
    "5. クラスタ中心の移動が起こらなければ終了．そうでなければ 3.~ 5. を繰り返す\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備，２次元のデータ生成\n",
    "\n",
    "とりあえず，トイデータとして２次元のサンプルを生成します．\n",
    "２次元の空間にデータをばらまいてクラスタリングさせるためのデータを乱数で作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 2次元のデータ生成\n",
    "np.random.seed(0)\n",
    "n_samples = 300\n",
    "n_features = 2\n",
    "n_clusters = 4\n",
    "c_std = 0.8\n",
    "\n",
    "# データ生成\n",
    "data, true_labels = make_blobs(n_samples=n_samples, centers=n_clusters, cluster_std=c_std, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを可視化するために下記の関数を定義し，描画してみます\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 結果の可視化\n",
    "def plot_clustering(data, labels, centroids=None, title=''):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "    \n",
    "    if centroids is not None:\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    \n",
    "# データの可視化\n",
    "plot_clustering(data, true_labels, title='True Clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データは実際には色分けされているわけではありませんが，`make_blob()` 関数でデータを作成していて，適当なクラスタからデータを生成しているので，各データに色をつけて提示しています．\n",
    "\n",
    "もちろん，機械学習でクラスタリングするときは，各クラスタの情報は提示しません．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-means 法によるクラスタリング\n",
    "\n",
    "K-means 法は自前で組んでもいいですが、scikit-learn に実装されているので、それを使います。\n",
    "手法としては `sklearn.cluster` モジュールの中にある `KMeans` クラスを用います．\n",
    "\n",
    "コンストラクタにわたす初期値としては\n",
    "\n",
    "- `n_clusters`: K平均法のクラスタ数 K の指定\n",
    "- `random_state`: 乱数の初期シード（クラスタ中心を乱数に従ってばらまくので指定）\n",
    "\n",
    "を指定したインスタンスを作成し，このインスタンスに対して `fit_predict()` 関数を用いることで実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# K-meansクラスタリングの適用\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0) # クラスタ数を指定してKMeansクラスのインスタンスを作成\n",
    "predicted_labels = kmeans.fit_predict(data) # クラスタリングを実行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだけだと，どうクラスタリングされたかわからないので，描画します．\n",
    "クラスタ情報は，`KMeans` クラスのインスタンス(上記例では `kmeans`) の属性である，`cluster_centers_` に入れられます．なので，この情報を用いて色付けを行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスタリング結果のプロット．先ほど定義した plot_clustering を利用\n",
    "plot_clustering(data, predicted_labels, centroids=kmeans.cluster_centers_, title=\"K-means Clustering\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微妙にずれてはいますが，大筋予想した通りに動いています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means 法の評価\n",
    "\n",
    "クラスタリングの良し悪しを測る場合，簡単な方法としてクラスタにどのくらいデータが集まっているか（クラスタ内凝集度 Within Cluster Sum of Squares: WCSS）を測る方法があります．\n",
    "この値は，scikit-learn の `KMeans` クラスの `inertia_` に記録されています．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WCSSの計算\n",
    "wcss = []\n",
    "k_values = range(1, 11)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, wcss, marker='o')\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Within-cluster Sum of Squares)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この曲線が変化が収束する点をクラスタ数の適切とする判断方法が Elbow 法と呼ばれる手法になります．この場合だと，おおよそ $k=4$ がその値になります．\n",
    "ただし，これだと，判断基準が曖昧になることもしばしば起きてきます．\n",
    "\n",
    "そこでクラスタの凝集度を測るほうほうとしてシルエットスコアを用いる方法もよく出てきます．シルエットスコアは\n",
    "`sklearn.metrics` の `silhouette_score()` 関数です．これはデータと，kmeans 法でクラスタ分けしたラベルを与えることで計算される値で，クラスタがどのくらい集まっていることと，他のクラスタとどの程度離れているかによって算出されます．\n",
    "値が大きいところが最適な $k$ の値となります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# シルエットスコアの計算\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 11)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    silhouette_scores.append(silhouette_score(data, labels))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだと，４個のクラスタ数が最適ということになります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験3-1 K-Means 法\n",
    "\n",
    "1. 適正なクラスタ数よりも少ない状況で k-means 法を用いた場合，クラスタの中心がどのような位置に配置されるかを図示し，その振る舞いを考察しなさい．\n",
    "2. 上述とは逆に過剰なクラスタ数を与えた状況で k-means 法を用いた場合，クラスタがどのように配置されるかを図示し，その振る舞いを考察しなさい．\n",
    "3. データ生成時のばらつき（`c_std` でコントロール）を 0.5 ~ 2 程度で変化させたとき，上記と同じ解析を行い，k-means 法の振る舞いについて考察しなさい．\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
